{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a4c6db67-4913-4e69-8048-c46531db44a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.3.1.post300)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.45.2)\n",
      "Requirement already satisfied: esm in /opt/conda/lib/python3.11/site-packages (3.0.6)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (0.24.5)\n",
      "Requirement already satisfied: tokenizers in /opt/conda/lib/python3.11/site-packages (0.20.1)\n",
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (from esm) (0.18.1a0+405940f)\n",
      "Requirement already satisfied: torchtext in /opt/conda/lib/python3.11/site-packages (from esm) (0.18.0)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.11/site-packages (from esm) (8.26.0)\n",
      "Requirement already satisfied: einops in /opt/conda/lib/python3.11/site-packages (from esm) (0.8.0)\n",
      "Requirement already satisfied: biotite==0.41.2 in /opt/conda/lib/python3.11/site-packages (from esm) (0.41.2)\n",
      "Requirement already satisfied: msgpack-numpy in /opt/conda/lib/python3.11/site-packages (from esm) (0.4.8)\n",
      "Requirement already satisfied: biopython in /opt/conda/lib/python3.11/site-packages (from esm) (1.84)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from esm) (1.4.2)\n",
      "Requirement already satisfied: brotli in /opt/conda/lib/python3.11/site-packages (from esm) (1.1.0)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.11/site-packages (from esm) (23.2.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from esm) (2.2.2)\n",
      "Requirement already satisfied: cloudpathlib in /opt/conda/lib/python3.11/site-packages (from esm) (0.18.1)\n",
      "Requirement already satisfied: tenacity in /opt/conda/lib/python3.11/site-packages (from esm) (8.5.0)\n",
      "Requirement already satisfied: msgpack>=0.5.6 in /opt/conda/lib/python3.11/site-packages (from biotite==0.41.2->esm) (1.0.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->esm) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->esm) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->esm) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->esm) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->esm) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->esm) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision->esm) (10.4.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython->esm) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython->esm) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->esm) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->esm) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->esm) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->esm) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->esm) (0.2.3)\n",
      "Downloading tiktoken-0.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers esm huggingface-hub tokenizers tiktoken\n",
    "!pip uninstall tensorflow -y\n",
    "TOKENIZERS_PARALLELISM = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "192968f4-d76c-420d-b2f7-421de03a1e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: fsspec\n",
      "Version: 2023.6.0\n",
      "Summary: File-system specification\n",
      "Home-page: http://github.com/fsspec/filesystem_spec\n",
      "Author: \n",
      "Author-email: \n",
      "License: BSD\n",
      "Location: /opt/conda/lib/python3.11/site-packages\n",
      "Requires: \n",
      "Required-by: dask, datasets, evaluate, huggingface_hub, jupyter_scheduler, lightning, PyAthena, pytorch-lightning, torch\n"
     ]
    }
   ],
   "source": [
    "!pip show fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eea715ce-015b-44cf-aea4-8ae1b4de8179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.45.2)\n",
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.3.1.post300)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5fb33c49-bceb-416b-85c4-3ed880d73231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import sklearn\n",
    "import esm\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a2da466d-b9bb-40dd-a142-1e17d65ca371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8cfa9e0-3c03-4411-b81e-39ba5f5d6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6fbe4769-37cf-4310-87fd-450a8d331362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "88c97687-d90e-4994-9407-2e23dba33df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESM3InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fa407da-150f-4a2b-a844-8bc1325c6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "529ce9a6-14ee-4645-b273-f9174c1e980b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e654d66da3b64889a6356cd79849846e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Set Persistent TORCH_HOME Directory Path\n",
    "efs_model_path = \"/home/sagemaker-user/user-default-efs/torch_hub\"\n",
    "\n",
    "# Step 2: Set TORCH_HOME Environment Variable to Ensure Persistent Storage\n",
    "os.environ['TORCH_HOME'] = efs_model_path\n",
    "if not os.path.exists(efs_model_path):\n",
    "    os.makedirs(efs_model_path)\n",
    "\n",
    "# Step 3: Set Device for Computation (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 4: Login to HuggingFace to Access ESM3 Weights\n",
    "# Optional optimization: Use `HUGGINGFACE_HUB_TOKEN` env variable to bypass manual login\n",
    "huggingface_token = os.getenv('HUGGINGFACE_HUB_TOKEN')\n",
    "if huggingface_token:\n",
    "    login(token=huggingface_token)\n",
    "else:\n",
    "    login()  # Will prompt user to enter their API key if no token is set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "3d1b7c9e-4d1a-4d1e-b023-d18a2ddf65f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Load ESM3 Model from HuggingFace Hub\n",
    "model: ESM3InferenceClient = ESM3.from_pretrained(\"esm3_sm_open_v1\").to(device)\n",
    "model = model.to(torch.float32)  # Convert to float32 if needed\n",
    "\n",
    "# Step 6: Save Model Weights to Persistent EFS Directory (optional, already stored via TORCH_HOME)\n",
    "# model.save_pretrained(efs_model_path)\n",
    "\n",
    "print(\"Model loaded and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "948dc618-469b-4de7-bc15-1e4fe077428d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Token IDs: [1, 9, 13, 21, 14, 2]\n",
      "Decoded Sequence: GLVM\n"
     ]
    }
   ],
   "source": [
    "class ProteinTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        \"\"\"\n",
    "        Initializes the ProteinTokenizer with a given vocabulary.\n",
    "\n",
    "        Args:\n",
    "            vocab (list): A list of strings representing the vocabulary, including special tokens and amino acids.\n",
    "        \"\"\"\n",
    "        # Create mappings from token to id and vice versa\n",
    "        self.token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
    "        self.id_to_token = {idx: token for token, idx in self.token_to_id.items()}\n",
    "\n",
    "    def tokenize(self, sequence):\n",
    "        \"\"\"\n",
    "        Tokenizes a protein sequence into individual characters.\n",
    "\n",
    "        Args:\n",
    "            sequence (str): A string representing the protein sequence.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of individual character tokens.\n",
    "        \"\"\"\n",
    "        return [char for char in sequence]\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"\n",
    "        Converts a list of tokens to their corresponding token IDs.\n",
    "\n",
    "        Args:\n",
    "            tokens (list): A list of character tokens.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of integer token IDs.\n",
    "        \"\"\"\n",
    "        return [self.token_to_id.get(token, self.token_to_id['[UNK]']) for token in tokens]\n",
    "\n",
    "    def encode(self, sequence, add_special_tokens=True):\n",
    "        \"\"\"\n",
    "        Encodes a protein sequence into token IDs.\n",
    "\n",
    "        Args:\n",
    "            sequence (str): A string representing the protein sequence.\n",
    "            add_special_tokens (bool): Whether to add special tokens like [CLS] and [SEP].\n",
    "\n",
    "        Returns:\n",
    "            list: A list of integer token IDs representing the input sequence.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(sequence)\n",
    "        token_ids = self.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # Optionally add special tokens like [CLS] at the beginning and [SEP] at the end\n",
    "        if add_special_tokens:\n",
    "            token_ids = [self.token_to_id['[CLS]']] + token_ids + [self.token_to_id['[SEP]']]\n",
    "\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=True):\n",
    "        \"\"\"\n",
    "        Decodes a list of token IDs back into a protein sequence.\n",
    "\n",
    "        Args:\n",
    "            token_ids (list): A list of integer token IDs.\n",
    "            skip_special_tokens (bool): Whether to skip special tokens during decoding.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded protein sequence.\n",
    "        \"\"\"\n",
    "        tokens = [self.id_to_token[token_id] for token_id in token_ids]\n",
    "        if skip_special_tokens:\n",
    "            tokens = [token for token in tokens if token not in ['[CLS]', '[SEP]', '[PAD]', '[UNK]']]\n",
    "        return ''.join(tokens)\n",
    "\n",
    "\n",
    "# Define the vocabulary: special tokens and amino acids\n",
    "special_tokens = ['[PAD]', '[CLS]', '[SEP]', '[UNK]']\n",
    "amino_acids = [\n",
    "    'A', 'C', 'D', 'E', 'F',\n",
    "    'G', 'H', 'I', 'K', 'L',\n",
    "    'M', 'N', 'P', 'Q', 'R',\n",
    "    'S', 'T', 'V', 'W', 'Y'\n",
    "]\n",
    "vocab = special_tokens + amino_acids\n",
    "\n",
    "# Initialize the custom tokenizer\n",
    "tokenizer = ProteinTokenizer(vocab=vocab)\n",
    "\n",
    "# Example usage\n",
    "sequence = \"GLVM\"\n",
    "encoded_sequence = tokenizer.encode(sequence)\n",
    "print(\"Encoded Token IDs:\", encoded_sequence)\n",
    "\n",
    "decoded_sequence = tokenizer.decode(encoded_sequence)\n",
    "print(\"Decoded Sequence:\", decoded_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0954cd1c-6862-4f64-9c83-d1c5cd9ab03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory exists: /home/sagemaker-user/user-default-efs/torch_hub\n",
      "Contents of /home/sagemaker-user/user-default-efs/torch_hub:\n",
      ".ipynb_checkpoints\n",
      "results_fold\n",
      "logs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "efs_model_path = \"/home/sagemaker-user/user-default-efs/torch_hub\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(efs_model_path):\n",
    "    print(f\"The directory exists: {efs_model_path}\")\n",
    "else:\n",
    "    print(f\"The directory does not exist: {efs_model_path}\")\n",
    "\n",
    "# List files and directories in the specified path\n",
    "files = os.listdir(efs_model_path)\n",
    "print(f\"Contents of {efs_model_path}:\")\n",
    "for f in files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "71eb8fd1-fb01-4782-8c75-3aa20024767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDB_Dataset(Dataset):\n",
    "    \"\"\"\n",
    "    A PyTorch Dataset class for handling protein sequences and contact numbers.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, tokenizer, label_type='regression', max_length=1022):\n",
    "        \"\"\"\n",
    "        Initializes the PDB_Dataset.\n",
    "        \n",
    "        Args:\n",
    "            df (pandas.DataFrame): DataFrame with two columns:\n",
    "                - Column 0: Protein sequences (str or list).\n",
    "                - Column 1: Contact number values (list).\n",
    "            tokenizer (ProteinTokenizer): Instance of the custom ProteinTokenizer.\n",
    "            label_type (str): 'regression' or 'binary'.\n",
    "            max_length (int): Maximum number of amino acids per sequence.\n",
    "        \"\"\"\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_type = label_type\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves the tokenized sequence and corresponding labels.\n",
    "        \n",
    "        Args:\n",
    "            idx (int): Index of the data point.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing 'sequence_tokens', 'attention_mask', and 'labels'.\n",
    "        \"\"\"\n",
    "        item = {}\n",
    "        \n",
    "        # Retrieve protein sequence\n",
    "        sequence = self.df.iloc[idx, 0]\n",
    "        if isinstance(sequence, list):\n",
    "            sequence = ''.join(sequence)\n",
    "        sequence = sequence.upper()  # Ensure uppercase letters\n",
    "\n",
    "        # Encode the sequence using the tokenizer\n",
    "        encoded = self.tokenizer.encode(sequence, add_special_tokens=True)\n",
    "        \n",
    "        # Truncate or pad the tokenized sequence\n",
    "        sequence_tokens = encoded\n",
    "        attention_mask = [1] * len(sequence_tokens)\n",
    "\n",
    "        # Define total length (including special tokens)\n",
    "        total_length = self.max_length + 2  # [CLS] and [SEP]\n",
    "        \n",
    "        if len(sequence_tokens) < total_length:\n",
    "            pad_length = total_length - len(sequence_tokens)\n",
    "            sequence_tokens += [self.tokenizer.token_to_id['[PAD]']] * pad_length\n",
    "            attention_mask += [0] * pad_length\n",
    "        else:\n",
    "            sequence_tokens = sequence_tokens[:total_length]\n",
    "            attention_mask = attention_mask[:total_length]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        item['sequence_tokens'] = torch.tensor(sequence_tokens, dtype=torch.long)\n",
    "        item['attention_mask'] = torch.tensor(attention_mask, dtype=torch.long)\n",
    "        \n",
    "        # Retrieve labels\n",
    "        labels = self.df.iloc[idx, 1]\n",
    "        \n",
    "        # Truncate or pad labels to match max_length\n",
    "        if len(labels) > self.max_length:\n",
    "            labels = labels[:self.max_length]\n",
    "        elif len(labels) < self.max_length:\n",
    "            pad_label = -100  # Commonly used to ignore in loss computation\n",
    "            labels += [pad_label] * (self.max_length - len(labels))\n",
    "        \n",
    "        # Convert labels to tensor\n",
    "        if self.label_type == 'regression':\n",
    "            labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        elif self.label_type == 'binary':\n",
    "            labels = torch.tensor(labels, dtype=torch.float32)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported label type: {self.label_type}\")\n",
    "        \n",
    "        item['labels'] = labels\n",
    "        \n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6a604baa-c8c8-45de-a343-abae7a69e7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ESM3ForTokenClassification(nn.Module):\n",
    "    \"\"\"\n",
    "    A custom model for token classification using ESM3.\n",
    "    \"\"\"\n",
    "    def __init__(self, model, num_labels=1):\n",
    "        \"\"\"\n",
    "        Initializes the ESM3ForTokenClassification model.\n",
    "        \n",
    "        Args:\n",
    "            model (ESM3InferenceClient): Pretrained ESM3 model instance.\n",
    "            num_labels (int): Number of labels for classification.\n",
    "        \"\"\"\n",
    "        super(ESM3ForTokenClassification, self).__init__()\n",
    "        self.esm3_model = model\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Determine hidden size from ESM3 model\n",
    "        hidden_size = 1536\n",
    "        \n",
    "        # Classification layer\n",
    "        self.classifier = nn.Linear(hidden_size, self.num_labels)\n",
    "    \n",
    "    def forward(self, sequence_tokens, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass for token classification.\n",
    "\n",
    "        Args:\n",
    "            sequence_tokens (torch.Tensor): The amino acid tokens.\n",
    "            attention_mask (torch.Tensor, optional): Attention masks.\n",
    "            labels (torch.Tensor, optional): Labels for classification.\n",
    "\n",
    "        Returns:\n",
    "            SequenceClassifierOutput: Contains loss and logits.\n",
    "        \"\"\"\n",
    "        # Pass the sequence tokens (instead of input_ids) to the ESM3 model\n",
    "        outputs = self.esm3_model(sequence_tokens=sequence_tokens)  # Assuming sequence_tokens are used\n",
    "\n",
    "        # Extract hidden states (adjust attribute based on ESM3's output)\n",
    "        if hasattr(outputs, 'last_hidden_state'):\n",
    "            hidden_states = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n",
    "        else:\n",
    "            hidden_states = outputs[0]  # Shape: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Pass hidden states through the classifier\n",
    "        logits = self.classifier(hidden_states)  # Shape: (batch_size, seq_len, num_labels)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                # Regression\n",
    "                loss_fct = nn.MSELoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            elif self.num_labels == 2:\n",
    "                # Binary classification (using BCEWithLogitsLoss)\n",
    "                loss_fct = nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                # Multi-class classification\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "0f39b45e-b30e-4ea0-a811-eb5695374936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init_1():\n",
    "    return ESM3ForTokenClassification(model=model, num_labels=1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c68c2bef-606d-438a-b6ca-7404e1d04376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Loss Function for Masked Regression\n",
    "class MaskedMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, target, mask):    \n",
    "        diff2 = (torch.flatten(inputs[:, :, 0]) - torch.flatten(target)) ** 2.0 * torch.flatten(mask)\n",
    "        result = torch.sum(diff2) / torch.sum(mask)\n",
    "        if torch.sum(mask) == 0:\n",
    "            return torch.sum(diff2)\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "0ce75ec3-444d-45d6-97a4-03cd8fcfdc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Masked Regress Trainer\n",
    "class MaskedRegressTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        labels = labels.squeeze().detach().cpu().numpy().tolist()\n",
    "        labels = [math.log(t + 1) if t != -100 else -100 for t in labels]\n",
    "        labels = torch.unsqueeze(torch.FloatTensor(labels), 0)\n",
    "        masks = ~torch.eq(labels, -100)\n",
    "\n",
    "        # Run the model on the input tokens\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Use a custom loss function\n",
    "        loss_fn = MaskedMSELoss()\n",
    "        loss = loss_fn(logits, labels, masks)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "543b54fe-db00-4e74-a075-e69c1559c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics Calculation Function\n",
    "def compute_metrics_regr(eval_pred: EvalPrediction):\n",
    "    preds = eval_pred.predictions[:, :, 0]  # Assuming num_labels=1\n",
    "    labels = eval_pred.label_ids\n",
    "    batch_size, seq_len = preds.shape\n",
    "    out_labels, out_preds = [], []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if labels[i, j] > -1:  # Assuming -100 is used for masked tokens\n",
    "                out_labels.append(labels[i, j])\n",
    "                out_preds.append(preds[i, j])\n",
    "\n",
    "    # Compute metrics\n",
    "    pearson_r = scipy.stats.pearsonr(out_labels, out_preds)[0]\n",
    "    mse = mean_squared_error(out_labels, out_preds)\n",
    "    r2 = r2_score(out_labels, out_preds)\n",
    "\n",
    "    return {\n",
    "        \"pearson_r\": pearson_r,\n",
    "        \"mse\": mse,\n",
    "        \"r2_score\": r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1434316d-81f6-4415-b3a5-5b29194b46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator Function for Batch Processing\n",
    "def collator_fn(batch):\n",
    "    if len(batch) == 1:\n",
    "        return batch[0]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ab7f862a-b006-4fba-9738-3743d476f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process the training set\n",
    "train_set = pd.read_csv('./data/sema_2.0/train_set.csv')\n",
    "train_set = train_set.groupby('pdb_id_chain').agg({\n",
    "    'resi_pos': list,\n",
    "    'resi_aa': list,\n",
    "    'contact_number': list\n",
    "}).reset_index()\n",
    "\n",
    "# Create training dataset\n",
    "train_ds = PDB_Dataset(\n",
    "    df=train_set[['resi_aa', 'contact_number']],\n",
    "    tokenizer=tokenizer,\n",
    "    label_type='regression',\n",
    "    max_length=1022  # Adjust as needed\n",
    ")\n",
    "\n",
    "# Load and process the test set\n",
    "test_set = pd.read_csv('./data/sema_2.0/test_set.csv')\n",
    "test_set = test_set.groupby('pdb_id_chain').agg({\n",
    "    'resi_pos': list,\n",
    "    'resi_aa': list,\n",
    "    'contact_number_binary': list\n",
    "}).reset_index()\n",
    "\n",
    "# Create test dataset\n",
    "test_ds = PDB_Dataset(\n",
    "    df=test_set[['resi_aa', 'contact_number_binary']],\n",
    "    tokenizer=tokenizer,\n",
    "    label_type='binary',  # Changed to 'binary' as per the test set\n",
    "    max_length=1022  # Ensure consistency with training set\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "12904eb0-d4b3-4f13-b2e6-fff509307cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 10: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(efs_model_path, 'results_fold'),           # Output directory\n",
    "    num_train_epochs=1,                    # Total number of training epochs\n",
    "    per_device_train_batch_size=1,         # Batch size per device during training\n",
    "    per_device_eval_batch_size=1,          # Batch size for evaluation\n",
    "    warmup_steps=0,                        # Number of warmup steps for learning rate scheduler\n",
    "    learning_rate=1e-05,                   # Learning rate\n",
    "    weight_decay=0.0,                      # Strength of weight decay\n",
    "    logging_dir=os.path.join(efs_model_path, 'logs'),                  # Directory for storing logs\n",
    "    logging_steps=200,                     # Log every 200 steps\n",
    "    save_strategy=\"no\",                    # Do not save checkpoints\n",
    "    do_train=True,                         # Perform training\n",
    "    do_eval=True,                          # Perform evaluation\n",
    "    eval_strategy=\"epoch\",           # Evaluate after each epoch\n",
    "    gradient_accumulation_steps=1,         # Number of steps before backpropagation\n",
    "    fp16=False,                            # Use mixed precision\n",
    "    run_name=\"PDB_regression\",             # Experiment name\n",
    "    seed=42,                               # Seed for reproducibility\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"r2_score\",\n",
    "    greater_is_better=True,\n",
    "    # Removed 'use_cpu' as it's not a valid argument in TrainingArguments\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8424a2bd-f92c-4056-8fec-e37998059b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_forward_check(model_instance, tokenizer, sequence=\"GLVM\", max_length=1024):\n",
    "    \"\"\"\n",
    "    Performs a dummy forward pass to verify model and tokenizer functionality.\n",
    "    \n",
    "    Args:\n",
    "        model_instance (ESM3ForTokenClassification): The custom model instance.\n",
    "        tokenizer (ProteinTokenizer): The tokenizer to encode the sequence.\n",
    "        sequence (str): The protein sequence to test.\n",
    "        max_length (int): Maximum sequence length.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Encode the sequence into token IDs using the tokenizer\n",
    "    sequence_tokens = tokenizer.encode(sequence)\n",
    "\n",
    "    # Define total length (including special tokens like [CLS] and [SEP])\n",
    "    total_length = max_length + 2\n",
    "    if len(sequence_tokens) < total_length:\n",
    "        pad_length = total_length - len(sequence_tokens)\n",
    "        sequence_tokens += [tokenizer.token_to_id['[PAD]']] * pad_length\n",
    "    else:\n",
    "        sequence_tokens = sequence_tokens[:total_length]\n",
    "\n",
    "    # Create attention mask\n",
    "    attention_mask = [1 if token_id != tokenizer.token_to_id['[PAD]'] else 0 for token_id in sequence_tokens]\n",
    "\n",
    "    # Convert to tensors\n",
    "    sequence_tokens = torch.tensor([sequence_tokens], dtype=torch.long).to(device)  # Shape: (1, seq_len)\n",
    "    attention_mask = torch.tensor([attention_mask], dtype=torch.long).to(device)    # Shape: (1, seq_len)\n",
    "\n",
    "    # Create dummy labels (for the sake of testing)\n",
    "    labels = torch.tensor([[0.0] * max_length], dtype=torch.float32).to(device)\n",
    "\n",
    "    # Forward pass with the ESM3 model\n",
    "    outputs = model_instance(sequence_tokens=sequence_tokens, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "    # Inspect the outputs\n",
    "    print(\"Type of outputs:\", type(outputs))\n",
    "    print(\"Attributes of outputs:\", dir(outputs))\n",
    "    print(\"Contents of outputs:\", outputs)\n",
    "\n",
    "    # Add dummy checks\n",
    "    assert hasattr(outputs, 'logits'), \"Output does not contain logits.\"\n",
    "\n",
    "    # Expected logits shape: (batch_size, seq_len, num_labels)\n",
    "    expected_shape = (1, total_length, model_instance.num_labels)\n",
    "    assert outputs.logits.shape == expected_shape, f\"Unexpected logits shape: {outputs.logits.shape}. Expected: {expected_shape}\"\n",
    "\n",
    "    print(\"Dummy checks passed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a692442e-be18-49d9-b290-9a84dc3dea2e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 66.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Call the dummy check function\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdummy_forward_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_init_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGLVM\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#1022\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[129], line 36\u001b[0m, in \u001b[0;36mdummy_forward_check\u001b[0;34m(model_instance, tokenizer, sequence, max_length)\u001b[0m\n\u001b[1;32m     33\u001b[0m labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;241m*\u001b[39m max_length], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Forward pass with the ESM3 model\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Inspect the outputs\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType of outputs:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtype\u001b[39m(outputs))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[115], line 36\u001b[0m, in \u001b[0;36mESM3ForTokenClassification.forward\u001b[0;34m(self, sequence_tokens, attention_mask, labels)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03mForward pass for token classification.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03m    SequenceClassifierOutput: Contains loss and logits.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Pass the sequence tokens (instead of input_ids) to the ESM3 model\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesm3_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msequence_tokens\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Assuming sequence_tokens are used\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Extract hidden states (adjust attribute based on ESM3's output)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(outputs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_hidden_state\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/esm/models/esm3.py:391\u001b[0m, in \u001b[0;36mESM3.forward\u001b[0;34m(self, sequence_tokens, structure_tokens, ss8_tokens, sasa_tokens, function_tokens, residue_annotation_tokens, average_plddt, per_res_plddt, structure_coords, chain_id, sequence_id)\u001b[0m\n\u001b[1;32m    370\u001b[0m structure_tokens \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    371\u001b[0m     structure_tokens\u001b[38;5;241m.\u001b[39mmasked_fill(structure_tokens \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, C\u001b[38;5;241m.\u001b[39mSTRUCTURE_MASK_TOKEN)\n\u001b[1;32m    372\u001b[0m     \u001b[38;5;241m.\u001b[39mmasked_fill(sequence_tokens \u001b[38;5;241m==\u001b[39m C\u001b[38;5;241m.\u001b[39mSEQUENCE_BOS_TOKEN, C\u001b[38;5;241m.\u001b[39mSTRUCTURE_BOS_TOKEN)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m     )\n\u001b[1;32m    379\u001b[0m )\n\u001b[1;32m    381\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    382\u001b[0m     sequence_tokens,\n\u001b[1;32m    383\u001b[0m     structure_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    389\u001b[0m     residue_annotation_tokens,\n\u001b[1;32m    390\u001b[0m )\n\u001b[0;32m--> 391\u001b[0m x, embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_heads(x, embedding)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/esm/layers/transformer_stack.py:89\u001b[0m, in \u001b[0;36mTransformerStack.forward\u001b[0;34m(self, x, sequence_id, affine, affine_mask, chain_id)\u001b[0m\n\u001b[1;32m     87\u001b[0m     chain_id \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(size\u001b[38;5;241m=\u001b[39mbatch_dims, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mint64, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[0;32m---> 89\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maffine_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x), x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/esm/layers/blocks.py:147\u001b[0m, in \u001b[0;36mUnifiedTransformerBlock.forward\u001b[0;34m(self, x, sequence_id, frames, frames_mask, chain_id)\u001b[0m\n\u001b[1;32m    144\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m r1 \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling_factor\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_geom_attn:\n\u001b[0;32m--> 147\u001b[0m     r2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeom_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m r2 \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling_factor\n\u001b[1;32m    150\u001b[0m r3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mffn(x) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaling_factor\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/esm/layers/geom_attention.py:108\u001b[0m, in \u001b[0;36mGeometricReasoningOriginalImpl.forward\u001b[0;34m(self, s, affine, affine_mask, sequence_id, chain_id)\u001b[0m\n\u001b[1;32m    103\u001b[0m value \u001b[38;5;241m=\u001b[39m rearrange(\n\u001b[1;32m    104\u001b[0m     value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb s (h m) d -> b h s (m d)\u001b[39m\u001b[38;5;124m\"\u001b[39m, m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_vector_messages\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    107\u001b[0m distance_term \u001b[38;5;241m=\u001b[39m (query_dist \u001b[38;5;241m-\u001b[39m key_dist)\u001b[38;5;241m.\u001b[39mnorm(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m sqrt(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m--> 108\u001b[0m rotation_term \u001b[38;5;241m=\u001b[39m \u001b[43mquery_rot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_rot\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m sqrt(\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m    109\u001b[0m distance_term_weight \u001b[38;5;241m=\u001b[39m rearrange(\n\u001b[1;32m    110\u001b[0m     F\u001b[38;5;241m.\u001b[39msoftplus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistance_scale_per_head), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh -> h 1 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m rotation_term_weight \u001b[38;5;241m=\u001b[39m rearrange(\n\u001b[1;32m    113\u001b[0m     F\u001b[38;5;241m.\u001b[39msoftplus(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotation_scale_per_head), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mh -> h 1 1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    114\u001b[0m )\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 66.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "# Call the dummy check function\n",
    "dummy_forward_check(model_init_1(), tokenizer, sequence=\"GLVM\", max_length=256) #1022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74c97779-a6f6-4499-8d47-8ed7a669e17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence tokenizer found.\n",
      "Encoded tokens (dummy): [6, 4, 7, 20]\n",
      "Embedding dimension: 1536\n"
     ]
    },
    {
     "ename": "NotAnAttrsClassError",
     "evalue": "<class 'str'> is not an attrs-decorated class.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotAnAttrsClassError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 12\u001b[0m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MaskedRegressTrainer(\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_init_1(),   \u001b[38;5;66;03m# Use the ESM3-based model for token classification\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics_regr\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Save the fine-tuned weights\u001b[39;00m\n\u001b[1;32m     15\u001b[0m fine_tuned_model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(efs_model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine_tuned_sema_3_ESM3.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:2052\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2050\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   2051\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2052\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2053\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2057\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/transformers/trainer.py:2345\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2342\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2344\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2345\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   2346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtotal_batched_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minclude_num_input_tokens_seen\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/accelerate/data_loader.py:550\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 550\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m, in \u001b[0;36mPDB_Dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Encode the sequence using the model's encode method (assuming it's available)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 25\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe provided model does not have an \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/esm/models/esm3.py:433\u001b[0m, in \u001b[0;36mESM3.encode\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: ESMProtein) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ESMProteinTensor:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mattr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevolve\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Make a copy\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     sequence_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     structure_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/attr/_funcs.py:410\u001b[0m, in \u001b[0;36mevolve\u001b[0;34m(*args, **changes)\u001b[0m\n\u001b[1;32m    402\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the instance per keyword argument is deprecated and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwill stop working in, or after, April 2024.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[1;32m    406\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    407\u001b[0m     )\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m inst\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[0;32m--> 410\u001b[0m attrs \u001b[38;5;241m=\u001b[39m \u001b[43mfields\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m attrs:\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m a\u001b[38;5;241m.\u001b[39minit:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/attr/_make.py:2055\u001b[0m, in \u001b[0;36mfields\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   2053\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m attrs\n\u001b[1;32m   2054\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m is not an attrs-decorated class.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 2055\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NotAnAttrsClassError(msg)\n\u001b[1;32m   2057\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attrs\n",
      "\u001b[0;31mNotAnAttrsClassError\u001b[0m: <class 'str'> is not an attrs-decorated class."
     ]
    }
   ],
   "source": [
    "# Step 11: Instantiate the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model_init_1(),   # Use the custom ESM3-based model for token classification\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    data_collator=collator_fn,\n",
    "    compute_metrics=compute_metrics_regr\n",
    ")\n",
    "\n",
    "# Step 12: Train the Model\n",
    "trainer.train()\n",
    "\n",
    "# Step 13: Save the Fine-Tuned Weights\n",
    "fine_tuned_model_path = os.path.join(efs_model_path, \"fine_tuned_sema_3_ESM3.pth\")\n",
    "torch.save(trainer.model.state_dict(), fine_tuned_model_path)\n",
    "print(f\"Fine-tuned model saved at {fine_tuned_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6887fef-7c0b-4353-9ed1-c40d8fa60fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
