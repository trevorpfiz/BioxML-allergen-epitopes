{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c6db67-4913-4e69-8048-c46531db44a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.3.1.post100)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.38.2)\n",
      "Collecting esm\n",
      "  Using cached esm-3.0.5-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (0.24.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (from esm) (0.18.1a0+405940f)\n",
      "Collecting torchtext (from esm)\n",
      "  Using cached torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.11/site-packages (from esm) (8.26.0)\n",
      "Collecting einops (from esm)\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting biotite==0.41.2 (from esm)\n",
      "  Using cached biotite-0.41.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.1 kB)\n",
      "Collecting msgpack-numpy (from esm)\n",
      "  Using cached msgpack_numpy-0.4.8-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting biopython (from esm)\n",
      "  Using cached biopython-1.84-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from esm) (1.4.2)\n",
      "Requirement already satisfied: brotli in /opt/conda/lib/python3.11/site-packages (from esm) (1.1.0)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.11/site-packages (from esm) (23.2.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from esm) (2.2.2)\n",
      "Requirement already satisfied: cloudpathlib in /opt/conda/lib/python3.11/site-packages (from esm) (0.18.1)\n",
      "Requirement already satisfied: tenacity in /opt/conda/lib/python3.11/site-packages (from esm) (8.5.0)\n",
      "Requirement already satisfied: msgpack>=0.5.6 in /opt/conda/lib/python3.11/site-packages (from biotite==0.41.2->esm) (1.0.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->esm) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->esm) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->esm) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->esm) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->esm) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->esm) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision->esm) (10.4.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython->esm) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython->esm) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->esm) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->esm) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->esm) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->esm) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->esm) (0.2.3)\n",
      "Using cached esm-3.0.5-py3-none-any.whl (148 kB)\n",
      "Using cached biotite-0.41.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.0 MB)\n",
      "Using cached biopython-1.84-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Using cached msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
      "Using cached torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
      "Installing collected packages: msgpack-numpy, einops, biopython, biotite, torchtext, esm\n",
      "Successfully installed biopython-1.84 biotite-0.41.2 einops-0.8.0 esm-3.0.5 msgpack-numpy-0.4.8 torchtext-0.18.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers esm huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb33c49-bceb-416b-85c4-3ed880d73231",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-13 05:18:45.627718: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy\n",
    "import sklearn\n",
    "import esm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "\n",
    "from esm.pretrained import load_model_and_alphabet_hub\n",
    "from huggingface_hub import login\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESM3InferenceClient\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529ce9a6-14ee-4645-b273-f9174c1e980b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m efs_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/sagemaker-user/user-default-efs/torch_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Step 2: Set TORCH_HOME Environment Variable to Ensure Persistent Storage\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTORCH_HOME\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m efs_model_path\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(efs_model_path):\n\u001b[1;32m      7\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(efs_model_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: Set Persistent TORCH_HOME Directory Path\n",
    "efs_model_path = \"/home/sagemaker-user/user-default-efs/torch_hub\"\n",
    "\n",
    "# Step 2: Set TORCH_HOME Environment Variable to Ensure Persistent Storage\n",
    "os.environ['TORCH_HOME'] = efs_model_path\n",
    "if not os.path.exists(efs_model_path):\n",
    "    os.makedirs(efs_model_path)\n",
    "\n",
    "# Step 3: Set Device for Computation (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 4: Login to HuggingFace to Access ESM3 Weights\n",
    "# Optional optimization: Use `HUGGINGFACE_HUB_TOKEN` env variable to bypass manual login\n",
    "huggingface_token = os.getenv('HUGGINGFACE_HUB_TOKEN')\n",
    "if huggingface_token:\n",
    "    login(token=huggingface_token)\n",
    "else:\n",
    "    login()  # Will prompt user to enter their API key if no token is set\n",
    "\n",
    "# Step 5: Load ESM3 Model from HuggingFace Hub\n",
    "model: ESM3InferenceClient = ESM3.from_pretrained(\"esm3_sm_open_v1\").to(device)\n",
    "\n",
    "# Step 6: Save Model Weights to Persistent EFS Directory (optional, already stored via TORCH_HOME)\n",
    "model.save_pretrained(efs_model_path)\n",
    "\n",
    "print(\"Model loaded and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9c2464-5a80-4d70-b285-95d9d465c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDB Dataset Class Update\n",
    "class PDB_Dataset(Dataset):\n",
    "    def __init__(self, df, label_type='regression'):\n",
    "        \"\"\"\n",
    "        Construct all the necessary attributes to the PDB_Dataset object.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pandas.DataFrame): dataframe with two columns: \n",
    "                0 -- protein sequence in string ('GLVM') or list (['G', 'L', 'V', 'M']) format\n",
    "                1 -- contact number values in list [0, 0.123, 0.23, -100, 1.34] format\n",
    "            label_type (str): type of model: regression or binary\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        # Use ESM3 Inference Client for token conversion\n",
    "        self.batch_converter = model.get_batch_converter()\n",
    "        self.label_type = label_type\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        _, _, tokens = self.batch_converter([('', ''.join(self.df.iloc[idx, 0])[:1024])])\n",
    "        item['token_ids'] = tokens\n",
    "        item['labels'] = torch.unsqueeze(torch.FloatTensor(self.df.iloc[idx, 1][:1024]), 0).to(torch.float32)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159b65b-b0e5-46a6-b266-2f557a4c2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESM3 Token Classification Model\n",
    "class ESM3ForTokenClassification(nn.Module):\n",
    "    def __init__(self, num_labels=1):\n",
    "        super().__init__()\n",
    "        # Load the ESM3 model\n",
    "        self.esm3 = model  # Use the ESM3InferenceClient loaded previously\n",
    "        self.num_labels = num_labels\n",
    "        # Define a linear classification layer to classify each token\n",
    "        self.classifier = nn.Linear(self.esm3.embed_dim, num_labels)\n",
    "\n",
    "    def forward(self, token_ids, labels=None):\n",
    "        # Forward pass using ESM3 to get representations\n",
    "        outputs = self.esm3.forward(token_ids)\n",
    "        # Get the representations for each token, ignoring CLS and padding tokens\n",
    "        hidden_states = outputs['representations'][:, 1:-1, :]  # Shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # Pass the representations through the classification layer\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        return SequenceClassifierOutput(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f39b45e-b30e-4ea0-a811-eb5695374936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init_1():\n",
    "    return ESM3ForTokenClassification(pretrained_no = 1)#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c2bef-606d-438a-b6ca-7404e1d04376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Loss Function for Masked Regression\n",
    "class MaskedMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, target, mask):    \n",
    "        diff2 = (torch.flatten(inputs[:, :, 0]) - torch.flatten(target)) ** 2.0 * torch.flatten(mask)\n",
    "        result = torch.sum(diff2) / torch.sum(mask)\n",
    "        if torch.sum(mask) == 0:\n",
    "            return torch.sum(diff2)\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce75ec3-444d-45d6-97a4-03cd8fcfdc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Masked Regress Trainer\n",
    "class MaskedRegressTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        labels = labels.squeeze().detach().cpu().numpy().tolist()\n",
    "        labels = [math.log(t + 1) if t != -100 else -100 for t in labels]\n",
    "        labels = torch.unsqueeze(torch.FloatTensor(labels), 0)\n",
    "        masks = ~torch.eq(labels, -100)\n",
    "\n",
    "        # Run the model on the input tokens\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Use a custom loss function\n",
    "        loss_fn = MaskedMSELoss()\n",
    "        loss = loss_fn(logits, labels, masks)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b54fe-db00-4e74-a075-e69c1559c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics Calculation Function\n",
    "def compute_metrics_regr(p: EvalPrediction):\n",
    "    preds = p.predictions[:, :, 0]\n",
    "    batch_size, seq_len = preds.shape\n",
    "    out_labels, out_preds = [], []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if p.label_ids[i, j] > -1:\n",
    "                out_labels.append(p.label_ids[i][j])\n",
    "                out_preds.append(preds[i][j])\n",
    "\n",
    "    return {\n",
    "        \"pearson_r\": scipy.stats.pearsonr(out_labels, out_preds)[0],\n",
    "        \"mse\": mean_squared_error(out_labels, out_preds),\n",
    "        \"r2_score\": r2_score(out_labels, out_preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1434316d-81f6-4415-b3a5-5b29194b46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator Function for Batch Processing\n",
    "def collator_fn(x):\n",
    "    if len(x) == 1:\n",
    "        return x[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7f862a-b006-4fba-9738-3743d476f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('./data/sema_2.0/train_set.csv')\n",
    "train_set = train_set.groupby('pdb_id_chain').agg({'resi_pos': list,\n",
    "                                 'resi_aa': list,\n",
    "                                 'contact_number': list}).reset_index()\n",
    "## the first run will take about 5-10 minutes, because esm weights should be downloaded\n",
    "train_ds = PDB_Dataset(train_set[['resi_aa', 'contact_number']], \n",
    "                      label_type ='regression')\n",
    "\n",
    "test_set = pd.read_csv('../data/sema_2.0/test_set.csv')\n",
    "test_set = test_set.groupby('pdb_id_chain').agg({'resi_pos': list,\n",
    "                                 'resi_aa': list,\n",
    "                                 'contact_number_binary': list}).reset_index()\n",
    "test_ds = PDB_Dataset(test_set[['resi_aa', 'contact_number_binary']],\n",
    "                      label_type ='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12904eb0-d4b3-4f13-b2e6-fff509307cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=efs_model_path + '/results_fold',           # output directory\n",
    "    num_train_epochs=2,                    # total number of training epochs\n",
    "    per_device_train_batch_size=1,         # batch size per device during training\n",
    "    per_device_eval_batch_size=1,          # batch size for evaluation\n",
    "    warmup_steps=0,                        # number of warmup steps for learning rate scheduler\n",
    "    learning_rate=1e-05,                   # learning rate\n",
    "    weight_decay=0.0,                      # strength of weight decay\n",
    "    logging_dir=efs_model_path + '/logs',                  # directory for storing logs\n",
    "    logging_steps=200,                     # log every 200 steps\n",
    "    save_strategy=\"no\",                    # do not save checkpoints\n",
    "    do_train=True,                         # Perform training\n",
    "    do_eval=True,                          # Perform evaluation\n",
    "    evaluation_strategy=\"epoch\",           # evaluate after each epoch\n",
    "    gradient_accumulation_steps=1,         # number of steps before backpropagation\n",
    "    fp16=False,                            # Use mixed precision\n",
    "    run_name=\"PDB_binary\",                 # experiment name\n",
    "    seed=42,                               # Seed for reproducibility\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    greater_is_better=True,\n",
    "    use_cpu=True\n",
    ")\n",
    "\n",
    "# Instantiate and train the model\n",
    "trainer = MaskedRegressTrainer(\n",
    "    model=ESM3ForTokenClassification(),   # Use the ESM3-based model for token classification\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    data_collator=collator_fn,\n",
    "    compute_metrics=compute_metrics_regr\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned weights\n",
    "fine_tuned_model_path = os.path.join(efs_model_path, \"fine_tuned_sema_3_ESM3.pth\")\n",
    "torch.save(trainer.model.state_dict(), fine_tuned_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
