{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c6db67-4913-4e69-8048-c46531db44a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch transformers esm huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb33c49-bceb-416b-85c4-3ed880d73231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import sklearn\n",
    "import esm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
    "\n",
    "from esm.pretrained import load_model_and_alphabet_hub\n",
    "from huggingface_hub import login\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESM3InferenceClient\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ce9a6-14ee-4645-b273-f9174c1e980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set Persistent TORCH_HOME Directory Path\n",
    "efs_model_path = \"/home/sagemaker-user/user-default-efs/torch_hub\"\n",
    "\n",
    "# Step 2: Set TORCH_HOME Environment Variable to Ensure Persistent Storage\n",
    "os.environ['TORCH_HOME'] = efs_model_path\n",
    "if not os.path.exists(efs_model_path):\n",
    "    os.makedirs(efs_model_path)\n",
    "\n",
    "# Step 3: Set Device for Computation (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 4: Login to HuggingFace to Access ESM3 Weights\n",
    "# Optional optimization: Use `HUGGINGFACE_HUB_TOKEN` env variable to bypass manual login\n",
    "huggingface_token = os.getenv('HUGGINGFACE_HUB_TOKEN')\n",
    "if huggingface_token:\n",
    "    login(token=huggingface_token)\n",
    "else:\n",
    "    login()  # Will prompt user to enter their API key if no token is set\n",
    "\n",
    "# Step 5: Load ESM3 Model from HuggingFace Hub\n",
    "model: ESM3InferenceClient = ESM3.from_pretrained(\"esm3_sm_open_v1\").to(device)\n",
    "\n",
    "# Step 6: Save Model Weights to Persistent EFS Directory (optional, already stored via TORCH_HOME)\n",
    "model.save_pretrained(efs_model_path)\n",
    "\n",
    "print(\"Model loaded and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9c2464-5a80-4d70-b285-95d9d465c240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDB Dataset Class Update\n",
    "class PDB_Dataset(Dataset):\n",
    "    def __init__(self, df, label_type='regression'):\n",
    "        \"\"\"\n",
    "        Construct all the necessary attributes to the PDB_Dataset object.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pandas.DataFrame): dataframe with two columns: \n",
    "                0 -- protein sequence in string ('GLVM') or list (['G', 'L', 'V', 'M']) format\n",
    "                1 -- contact number values in list [0, 0.123, 0.23, -100, 1.34] format\n",
    "            label_type (str): type of model: regression or binary\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        # Use ESM3 Inference Client for token conversion\n",
    "        self.batch_converter = model.get_batch_converter()\n",
    "        self.label_type = label_type\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "        _, _, tokens = self.batch_converter([('', ''.join(self.df.iloc[idx, 0])[:1024])])\n",
    "        item['token_ids'] = tokens\n",
    "        item['labels'] = torch.unsqueeze(torch.FloatTensor(self.df.iloc[idx, 1][:1024]), 0).to(torch.float32)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d159b65b-b0e5-46a6-b266-2f557a4c2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESM3 Token Classification Model\n",
    "class ESM3ForTokenClassification(nn.Module):\n",
    "    def __init__(self, num_labels=1):\n",
    "        super().__init__()\n",
    "        # Load the ESM3 model\n",
    "        self.esm3 = model  # Use the ESM3InferenceClient loaded previously\n",
    "        self.num_labels = num_labels\n",
    "        # Define a linear classification layer to classify each token\n",
    "        self.classifier = nn.Linear(self.esm3.embed_dim, num_labels)\n",
    "\n",
    "    def forward(self, token_ids, labels=None):\n",
    "        # Forward pass using ESM3 to get representations\n",
    "        outputs = self.esm3.forward(token_ids)\n",
    "        # Get the representations for each token, ignoring CLS and padding tokens\n",
    "        hidden_states = outputs['representations'][:, 1:-1, :]  # Shape: (batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        # Pass the representations through the classification layer\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        return SequenceClassifierOutput(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68c2bef-606d-438a-b6ca-7404e1d04376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Loss Function for Masked Regression\n",
    "class MaskedMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, target, mask):    \n",
    "        diff2 = (torch.flatten(inputs[:, :, 0]) - torch.flatten(target)) ** 2.0 * torch.flatten(mask)\n",
    "        result = torch.sum(diff2) / torch.sum(mask)\n",
    "        if torch.sum(mask) == 0:\n",
    "            return torch.sum(diff2)\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce75ec3-444d-45d6-97a4-03cd8fcfdc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Masked Regress Trainer\n",
    "class MaskedRegressTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        labels = labels.squeeze().detach().cpu().numpy().tolist()\n",
    "        labels = [math.log(t + 1) if t != -100 else -100 for t in labels]\n",
    "        labels = torch.unsqueeze(torch.FloatTensor(labels), 0)\n",
    "        masks = ~torch.eq(labels, -100)\n",
    "\n",
    "        # Run the model on the input tokens\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Use a custom loss function\n",
    "        loss_fn = MaskedMSELoss()\n",
    "        loss = loss_fn(logits, labels, masks)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b54fe-db00-4e74-a075-e69c1559c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics Calculation Function\n",
    "def compute_metrics_regr(p: EvalPrediction):\n",
    "    preds = p.predictions[:, :, 0]\n",
    "    batch_size, seq_len = preds.shape\n",
    "    out_labels, out_preds = [], []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if p.label_ids[i, j] > -1:\n",
    "                out_labels.append(p.label_ids[i][j])\n",
    "                out_preds.append(preds[i][j])\n",
    "\n",
    "    return {\n",
    "        \"pearson_r\": scipy.stats.pearsonr(out_labels, out_preds)[0],\n",
    "        \"mse\": mean_squared_error(out_labels, out_preds),\n",
    "        \"r2_score\": r2_score(out_labels, out_preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1434316d-81f6-4415-b3a5-5b29194b46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator Function for Batch Processing\n",
    "def collator_fn(x):\n",
    "    if len(x) == 1:\n",
    "        return x[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7f862a-b006-4fba-9738-3743d476f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('./data/sema_2.0/train_set.csv')\n",
    "train_set = train_set.groupby('pdb_id_chain').agg({'resi_pos': list,\n",
    "                                 'resi_aa': list,\n",
    "                                 'contact_number': list}).reset_index()\n",
    "## the first run will take about 5-10 minutes, because esm weights should be downloaded\n",
    "train_ds = PDB_Dataset(train_set[['resi_aa', 'contact_number']], \n",
    "                      label_type ='regression')\n",
    "\n",
    "test_set = pd.read_csv('../data/sema_2.0/test_set.csv')\n",
    "test_set = test_set.groupby('pdb_id_chain').agg({'resi_pos': list,\n",
    "                                 'resi_aa': list,\n",
    "                                 'contact_number_binary': list}).reset_index()\n",
    "test_ds = PDB_Dataset(test_set[['resi_aa', 'contact_number_binary']],\n",
    "                      label_type ='regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12904eb0-d4b3-4f13-b2e6-fff509307cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=efs_model_path + '/results_fold',           # output directory\n",
    "    num_train_epochs=2,                    # total number of training epochs\n",
    "    per_device_train_batch_size=1,         # batch size per device during training\n",
    "    per_device_eval_batch_size=1,          # batch size for evaluation\n",
    "    warmup_steps=0,                        # number of warmup steps for learning rate scheduler\n",
    "    learning_rate=1e-05,                   # learning rate\n",
    "    weight_decay=0.0,                      # strength of weight decay\n",
    "    logging_dir=efs_model_path + '/logs',                  # directory for storing logs\n",
    "    logging_steps=200,                     # log every 200 steps\n",
    "    save_strategy=\"no\",                    # do not save checkpoints\n",
    "    do_train=True,                         # Perform training\n",
    "    do_eval=True,                          # Perform evaluation\n",
    "    evaluation_strategy=\"epoch\",           # evaluate after each epoch\n",
    "    gradient_accumulation_steps=1,         # number of steps before backpropagation\n",
    "    fp16=False,                            # Use mixed precision\n",
    "    run_name=\"PDB_binary\",                 # experiment name\n",
    "    seed=42,                               # Seed for reproducibility\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    greater_is_better=True,\n",
    "    use_cpu=True\n",
    ")\n",
    "\n",
    "# Instantiate and train the model\n",
    "trainer = MaskedRegressTrainer(\n",
    "    model=ESM3ForTokenClassification(),   # Use the ESM3-based model for token classification\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    data_collator=collator_fn,\n",
    "    compute_metrics=compute_metrics_regr\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned weights\n",
    "fine_tuned_model_path = os.path.join(efs_model_path, \"fine_tuned_sema_3_ESM3.pth\")\n",
    "torch.save(trainer.model.state_dict(), fine_tuned_model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
