{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c6db67-4913-4e69-8048-c46531db44a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.3.1.post300)\n",
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.11/site-packages (4.38.2)\n",
      "Collecting esm\n",
      "  Using cached esm-3.0.5-py3-none-any.whl.metadata (9.4 kB)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.11/site-packages (0.24.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (2024.7.24)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (from esm) (0.18.1a0+405940f)\n",
      "Collecting torchtext (from esm)\n",
      "  Using cached torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.11/site-packages (from esm) (8.26.0)\n",
      "Collecting einops (from esm)\n",
      "  Using cached einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting biotite==0.41.2 (from esm)\n",
      "  Using cached biotite-0.41.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.1 kB)\n",
      "Collecting msgpack-numpy (from esm)\n",
      "  Using cached msgpack_numpy-0.4.8-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting biopython (from esm)\n",
      "  Using cached biopython-1.84-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (from esm) (1.4.2)\n",
      "Requirement already satisfied: brotli in /opt/conda/lib/python3.11/site-packages (from esm) (1.1.0)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.11/site-packages (from esm) (23.2.0)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (from esm) (2.2.2)\n",
      "Requirement already satisfied: cloudpathlib in /opt/conda/lib/python3.11/site-packages (from esm) (0.18.1)\n",
      "Requirement already satisfied: tenacity in /opt/conda/lib/python3.11/site-packages (from esm) (8.5.0)\n",
      "Requirement already satisfied: msgpack>=0.5.6 in /opt/conda/lib/python3.11/site-packages (from biotite==0.41.2->esm) (1.0.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.7.4)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (3.0.47)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (0.6.2)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (5.14.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.11/site-packages (from ipython->esm) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas->esm) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas->esm) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas->esm) (2024.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->esm) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->esm) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn->esm) (3.5.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision->esm) (10.4.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /opt/conda/lib/python3.11/site-packages (from jedi>=0.16->ipython->esm) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.11/site-packages (from pexpect>4.3->ipython->esm) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython->esm) (0.2.13)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->esm) (1.16.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->esm) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->esm) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /opt/conda/lib/python3.11/site-packages (from stack-data->ipython->esm) (0.2.3)\n",
      "Using cached esm-3.0.5-py3-none-any.whl (148 kB)\n",
      "Using cached biotite-0.41.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (37.0 MB)\n",
      "Using cached biopython-1.84-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "Using cached einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "Using cached msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
      "Using cached torchtext-0.18.0-cp311-cp311-manylinux1_x86_64.whl (2.0 MB)\n",
      "Installing collected packages: msgpack-numpy, einops, biopython, biotite, torchtext, esm\n",
      "Successfully installed biopython-1.84 biotite-0.41.2 einops-0.8.0 esm-3.0.5 msgpack-numpy-0.4.8 torchtext-0.18.0\n",
      "Found existing installation: tensorflow 2.17.0\n",
      "Uninstalling tensorflow-2.17.0:\n",
      "  Successfully uninstalled tensorflow-2.17.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch transformers esm huggingface-hub\n",
    "!pip uninstall tensorflow -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb33c49-bceb-416b-85c4-3ed880d73231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy\n",
    "import sklearn\n",
    "import esm\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2da466d-b9bb-40dd-a142-1e17d65ca371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8cfa9e0-3c03-4411-b81e-39ba5f5d6a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fbe4769-37cf-4310-87fd-450a8d331362",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import Trainer, TrainingArguments, EvalPrediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88c97687-d90e-4994-9407-2e23dba33df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.sdk.api import ESM3InferenceClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fa407da-150f-4a2b-a844-8bc1325c6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "529ce9a6-14ee-4645-b273-f9174c1e980b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6d610edcf548b8bd934eafe8a4ef5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 1: Set Persistent TORCH_HOME Directory Path\n",
    "efs_model_path = \"/home/sagemaker-user/user-default-efs/torch_hub\"\n",
    "\n",
    "# Step 2: Set TORCH_HOME Environment Variable to Ensure Persistent Storage\n",
    "os.environ['TORCH_HOME'] = efs_model_path\n",
    "if not os.path.exists(efs_model_path):\n",
    "    os.makedirs(efs_model_path)\n",
    "\n",
    "# Step 3: Set Device for Computation (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Step 4: Login to HuggingFace to Access ESM3 Weights\n",
    "# Optional optimization: Use `HUGGINGFACE_HUB_TOKEN` env variable to bypass manual login\n",
    "huggingface_token = os.getenv('HUGGINGFACE_HUB_TOKEN')\n",
    "if huggingface_token:\n",
    "    login(token=huggingface_token)\n",
    "else:\n",
    "    login()  # Will prompt user to enter their API key if no token is set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d1b7c9e-4d1a-4d1e-b023-d18a2ddf65f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eca31d59c1044f98b382063feaf50875",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 22 files:   0%|          | 0/22 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded and saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Load ESM3 Model from HuggingFace Hub\n",
    "model: ESM3InferenceClient = ESM3.from_pretrained(\"esm3_sm_open_v1\").to(\"cuda\")\n",
    "\n",
    "# Step 6: Save Model Weights to Persistent EFS Directory (optional, already stored via TORCH_HOME)\n",
    "# model.save_pretrained(efs_model_path)\n",
    "\n",
    "print(\"Model loaded and saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0954cd1c-6862-4f64-9c83-d1c5cd9ab03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The directory exists: /home/sagemaker-user/user-default-efs/torch_hub\n",
      "Contents of /home/sagemaker-user/user-default-efs/torch_hub:\n",
      "Test file successfully written to: /home/sagemaker-user/user-default-efs/torch_hub/test_file.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "efs_model_path = \"/home/sagemaker-user/user-default-efs/torch_hub\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(efs_model_path):\n",
    "    print(f\"The directory exists: {efs_model_path}\")\n",
    "else:\n",
    "    print(f\"The directory does not exist: {efs_model_path}\")\n",
    "\n",
    "# List files and directories in the specified path\n",
    "files = os.listdir(efs_model_path)\n",
    "print(f\"Contents of {efs_model_path}:\")\n",
    "for f in files:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71eb8fd1-fb01-4782-8c75-3aa20024767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PDB_Dataset(Dataset):\n",
    "    def __init__(self, df, model, label_type='regression'):\n",
    "        \"\"\"\n",
    "        Construct all the necessary attributes for the PDB_Dataset object.\n",
    "        \n",
    "        Parameters:\n",
    "            df (pandas.DataFrame): dataframe with two columns: \n",
    "                0 -- protein sequence in string ('GLVM') or list (['G', 'L', 'V', 'M']) format\n",
    "                1 -- contact number values in list [0, 0.123, 0.23, -100, 1.34] format\n",
    "            model: The model object used for encoding sequences.\n",
    "            label_type (str): type of model: regression or binary classification\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.model = model\n",
    "        self.label_type = label_type\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {}\n",
    "\n",
    "        # Retrieve and prepare the sequence\n",
    "        sequence = ''.join(self.df.iloc[idx, 0])[:1024]  # Take up to 1024 residues\n",
    "\n",
    "        # Encode the sequence using the model's encode method (assuming it's available)\n",
    "        try:\n",
    "            tokens = self.model.encode(sequence)\n",
    "        except AttributeError:\n",
    "            raise ValueError(\"The provided model does not have an 'encode' method.\")\n",
    "\n",
    "        # Debug print to check encoding\n",
    "        print(f\"Index: {idx}, Sequence: {sequence}, Tokens: {tokens}\")\n",
    "\n",
    "        item['token_ids'] = tokens\n",
    "\n",
    "        # Prepare labels\n",
    "        label_values = self.df.iloc[idx, 1][:1024]  # Ensure label matches length limit of tokens\n",
    "        item['labels'] = torch.unsqueeze(torch.FloatTensor(label_values), 0).to(torch.float32)\n",
    "\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d159b65b-b0e5-46a6-b266-2f557a4c2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESM3 Token Classification Model\n",
    "class ESM3ForTokenClassification(nn.Module):\n",
    "    def __init__(self, model, num_labels=1):\n",
    "        \"\"\"\n",
    "        Initializes the ESM3ForTokenClassification model.\n",
    "\n",
    "        Args:\n",
    "            model (ESM3InferenceClient): The preloaded ESM3 model.\n",
    "            num_labels (int): Number of classification labels.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.esm3 = model.to(self.device)  # Move model to device\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "        # Access the EsmSequenceTokenizer from the TokenizerCollection\n",
    "        if hasattr(self.esm3, 'tokenizers') and hasattr(self.esm3.tokenizers, 'sequence'):\n",
    "            self.tokenizer = self.esm3.tokenizers.sequence\n",
    "            print(\"Sequence tokenizer found.\")\n",
    "        else:\n",
    "            raise AttributeError(\"Model does not have a 'sequence' tokenizer in 'tokenizers' attribute.\")\n",
    "\n",
    "        # Tokenize a dummy input to get the token IDs (for informational purposes)\n",
    "        dummy_sequence = (\"\", \"GLVM\")  # Tuple format as expected by ESM tokenizer\n",
    "        tokens = self.tokenizer.encode(dummy_sequence)\n",
    "        print(f\"Encoded tokens (dummy): {tokens}\")\n",
    "\n",
    "        # Retrieve embedding dimension from model's parameters\n",
    "        try:\n",
    "            embed_dim = next(self.esm3.parameters()).size(-1)\n",
    "            print(f\"Embedding dimension: {embed_dim}\")\n",
    "        except StopIteration:\n",
    "            raise ValueError(\"Model has no parameters to infer embedding dimension.\")\n",
    "\n",
    "        # Define a linear classification layer to classify each token\n",
    "        self.classifier = nn.Linear(embed_dim, num_labels).to(self.device)\n",
    "\n",
    "    def forward(self, sequences, labels=None):\n",
    "        \"\"\"\n",
    "        Forward pass for token classification.\n",
    "\n",
    "        Args:\n",
    "            sequences (List[str]): List of protein sequences.\n",
    "            labels (torch.Tensor, optional): Labels for the sequences.\n",
    "\n",
    "        Returns:\n",
    "            SequenceClassifierOutput: The classification logits.\n",
    "        \"\"\"\n",
    "        logits_list = []\n",
    "\n",
    "        for seq in sequences:\n",
    "            # Encode the sequence\n",
    "            encoded = self.tokenizer.encode((\"\", seq))\n",
    "            tokens_tensor = torch.tensor(encoded).unsqueeze(0).to(self.device)  # Shape: (1, seq_len)\n",
    "\n",
    "            # Create average_plddt tensor\n",
    "            model_dtype = next(self.esm3.parameters()).dtype\n",
    "            average_plddt = torch.tensor([70.0], dtype=model_dtype).to(self.device)  # Shape: (1,)\n",
    "\n",
    "            # Forward pass\n",
    "            try:\n",
    "                outputs = self.esm3(sequence_tokens=tokens_tensor, average_plddt=average_plddt)\n",
    "            except Exception as e:\n",
    "                raise RuntimeError(f\"Failed to perform a forward pass for sequence '{seq}': {e}\")\n",
    "\n",
    "            # Access the 'embeddings' attribute\n",
    "            if hasattr(outputs, 'embeddings'):\n",
    "                x = outputs.embeddings  # Shape: (1, seq_len, hidden_dim)\n",
    "            else:\n",
    "                raise AttributeError(\"ESMOutput does not have an 'embeddings' attribute.\")\n",
    "\n",
    "            # Pass through the classification layer\n",
    "            logits = self.classifier(x)  # Shape: (1, seq_len, num_labels)\n",
    "            logits_list.append(logits)\n",
    "\n",
    "        # Concatenate logits from all sequences\n",
    "        logits = torch.cat(logits_list, dim=0)  # Shape: (batch_size, seq_len, num_labels)\n",
    "\n",
    "        return SequenceClassifierOutput(logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "93cccac5-2b90-4efc-b443-41ff6fb17b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'esm.tokenization.TokenizerCollection'>\n",
      "TokenizerCollection(sequence=EsmSequenceTokenizer(name_or_path='', vocab_size=33, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<cls>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'cls_token': '<cls>', 'mask_token': '<mask>', 'additional_special_tokens': ['|']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<cls>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t2: AddedToken(\"<eos>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t3: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t31: AddedToken(\"|\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t32: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "}, structure=<esm.tokenization.structure_tokenizer.StructureTokenizer object at 0x7fe944dc7e50>, secondary_structure=<esm.tokenization.ss_tokenizer.SecondaryStructureTokenizer object at 0x7fe944f51710>, sasa=<esm.tokenization.sasa_tokenizer.SASADiscretizingTokenizer object at 0x7fe944f53b50>, function=<esm.tokenization.function_tokenizer.InterProQuantizedTokenizer object at 0x7fe944f53a90>, residue_annotations=<esm.tokenization.residue_tokenizer.ResidueAnnotationsTokenizer object at 0x7fe944f4a0d0>)\n"
     ]
    }
   ],
   "source": [
    "print(type(model.tokenizers))\n",
    "print(model.tokenizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f39b45e-b30e-4ea0-a811-eb5695374936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init_1():\n",
    "    return ESM3ForTokenClassification(model=model, num_labels=1)#.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c68c2bef-606d-438a-b6ca-7404e1d04376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Loss Function for Masked Regression\n",
    "class MaskedMSELoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MaskedMSELoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, target, mask):    \n",
    "        diff2 = (torch.flatten(inputs[:, :, 0]) - torch.flatten(target)) ** 2.0 * torch.flatten(mask)\n",
    "        result = torch.sum(diff2) / torch.sum(mask)\n",
    "        if torch.sum(mask) == 0:\n",
    "            return torch.sum(diff2)\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ce75ec3-444d-45d6-97a4-03cd8fcfdc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Masked Regress Trainer\n",
    "class MaskedRegressTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        labels = labels.squeeze().detach().cpu().numpy().tolist()\n",
    "        labels = [math.log(t + 1) if t != -100 else -100 for t in labels]\n",
    "        labels = torch.unsqueeze(torch.FloatTensor(labels), 0)\n",
    "        masks = ~torch.eq(labels, -100)\n",
    "\n",
    "        # Run the model on the input tokens\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Use a custom loss function\n",
    "        loss_fn = MaskedMSELoss()\n",
    "        loss = loss_fn(logits, labels, masks)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "543b54fe-db00-4e74-a075-e69c1559c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics Calculation Function\n",
    "def compute_metrics_regr(p: EvalPrediction):\n",
    "    preds = p.predictions[:, :, 0]\n",
    "    batch_size, seq_len = preds.shape\n",
    "    out_labels, out_preds = [], []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        for j in range(seq_len):\n",
    "            if p.label_ids[i, j] > -1:\n",
    "                out_labels.append(p.label_ids[i][j])\n",
    "                out_preds.append(preds[i][j])\n",
    "\n",
    "    return {\n",
    "        \"pearson_r\": scipy.stats.pearsonr(out_labels, out_preds)[0],\n",
    "        \"mse\": mean_squared_error(out_labels, out_preds),\n",
    "        \"r2_score\": r2_score(out_labels, out_preds)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1434316d-81f6-4415-b3a5-5b29194b46dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Collator Function for Batch Processing\n",
    "def collator_fn(x):\n",
    "    if len(x) == 1:\n",
    "        return x[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab7f862a-b006-4fba-9738-3743d476f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('./data/sema_2.0/train_set.csv')\n",
    "train_set = train_set.groupby('pdb_id_chain').agg({\n",
    "    'resi_pos': list,\n",
    "    'resi_aa': list,\n",
    "    'contact_number': list\n",
    "}).reset_index()\n",
    "\n",
    "# Create training dataset\n",
    "train_ds = PDB_Dataset(train_set[['resi_aa', 'contact_number']], model=model, label_type='regression')\n",
    "\n",
    "# Load and process the test set similarly\n",
    "test_set = pd.read_csv('./data/sema_2.0/test_set.csv')\n",
    "test_set = test_set.groupby('pdb_id_chain').agg({\n",
    "    'resi_pos': list,\n",
    "    'resi_aa': list,\n",
    "    'contact_number_binary': list\n",
    "}).reset_index()\n",
    "\n",
    "# Create test dataset\n",
    "test_ds = PDB_Dataset(test_set[['resi_aa', 'contact_number_binary']], model=model, label_type='regression')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "12904eb0-d4b3-4f13-b2e6-fff509307cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=efs_model_path + '/results_fold',           # output directory\n",
    "    num_train_epochs=2,                    # total number of training epochs\n",
    "    per_device_train_batch_size=1,         # batch size per device during training\n",
    "    per_device_eval_batch_size=1,          # batch size for evaluation\n",
    "    warmup_steps=0,                        # number of warmup steps for learning rate scheduler\n",
    "    learning_rate=1e-05,                   # learning rate\n",
    "    weight_decay=0.0,                      # strength of weight decay\n",
    "    logging_dir=efs_model_path + '/logs',                  # directory for storing logs\n",
    "    logging_steps=200,                     # log every 200 steps\n",
    "    save_strategy=\"no\",                    # do not save checkpoints\n",
    "    do_train=True,                         # Perform training\n",
    "    do_eval=True,                          # Perform evaluation\n",
    "    evaluation_strategy=\"epoch\",           # evaluate after each epoch\n",
    "    gradient_accumulation_steps=1,         # number of steps before backpropagation\n",
    "    fp16=False,                            # Use mixed precision\n",
    "    run_name=\"PDB_binary\",                 # experiment name\n",
    "    seed=42,                               # Seed for reproducibility\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"eval_accuracy\",\n",
    "    greater_is_better=True,\n",
    "    use_cpu=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8424a2bd-f92c-4056-8fec-e37998059b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence tokenizer found.\n",
      "Encoded tokens (dummy): [6, 4, 7, 20]\n",
      "Embedding dimension: 1536\n",
      "Type of outputs: <class 'esm.models.esm3.ESMOutput'>\n",
      "Attributes of outputs: ['__annotations__', '__attrs_attrs__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'embeddings', 'function_logits', 'residue_logits', 'sasa_logits', 'secondary_structure_logits', 'sequence_logits', 'structure_logits']\n",
      "Contents of outputs: ESMOutput(sequence_logits=tensor([[[-14.7519, -14.8172, -14.7101, -14.7850,  -3.8522,   1.1576,   9.8740,\n",
      "            0.7219,   3.7604,   1.1795,  -0.2083,  -2.4984,  -2.9204,   1.4685,\n",
      "           -2.5869,  -2.4647,  -3.8069,  -1.1124,  -4.1094,  -4.7673,  -4.7467,\n",
      "           -4.6795,  -3.5911,   0.0976,  -9.3138, -21.2335, -27.8444, -17.1927,\n",
      "          -14.2109, -14.6898, -14.6492, -14.9560, -14.6618, -14.6285, -14.4685,\n",
      "          -14.7067, -14.7315, -14.5944, -14.8616, -14.7934, -14.6803, -14.7233,\n",
      "          -14.8071, -14.6599, -14.7996, -14.7487, -14.6622, -14.6368, -14.6985,\n",
      "          -14.7480, -14.7482, -14.7734, -15.0138, -14.6206, -14.7728, -14.7801,\n",
      "          -14.9535, -14.7948, -14.9790, -14.8463, -14.6576, -14.8804, -14.6840,\n",
      "          -14.8126],\n",
      "         [-18.3297, -18.2252, -18.2431, -18.2070,   9.5346,  -1.6172,  -2.0536,\n",
      "            1.8395,   0.1541,  -3.5636,  -0.3996,  -2.0703,   1.1885,  -4.1437,\n",
      "           -0.8587,  -3.4832,  -0.1589,  -4.0287,   0.1931,  -3.3748,   0.9240,\n",
      "           -3.2178,  -2.5767,  -2.5895,  -9.3276, -26.2846, -22.4761, -22.2564,\n",
      "          -17.4863, -18.1930, -18.3352, -18.2127, -18.1524, -18.1729, -18.0420,\n",
      "          -18.1174, -18.1346, -18.0650, -18.0799, -18.1687, -18.0377, -17.9635,\n",
      "          -18.1189, -18.0606, -18.0254, -18.2345, -18.0596, -18.2102, -18.2801,\n",
      "          -18.0371, -18.2865, -18.0672, -18.1668, -18.2027, -18.1068, -18.0778,\n",
      "          -18.2835, -18.1366, -18.0150, -18.2888, -18.3143, -18.1977, -18.2097,\n",
      "          -18.2755],\n",
      "         [-18.4043, -18.2169, -18.4138, -18.2836,  -0.1889,   3.1533,   0.3542,\n",
      "            8.9721,  -1.4293,  -0.7102,  -3.9469,  -0.6051,   0.9310,   0.2583,\n",
      "           -2.2729,  -4.0754,  -3.0189,  -1.9635,   0.0755,  -1.3188,  -1.5240,\n",
      "           -4.4176,  -5.2272,  -2.4899,  -9.5867, -23.1356, -23.5792, -20.9848,\n",
      "          -17.7632, -18.3776, -18.1746, -18.2330, -18.3612, -18.1616, -18.1364,\n",
      "          -18.1816, -18.1644, -18.1587, -18.3074, -18.3739, -18.2919, -18.1438,\n",
      "          -18.2733, -18.2709, -18.1952, -18.3752, -18.2582, -18.1197, -18.2344,\n",
      "          -18.2633, -18.3020, -18.1037, -18.4410, -18.3210, -18.3624, -18.2486,\n",
      "          -18.4112, -18.2277, -18.3109, -18.1689, -18.1803, -18.2163, -18.3138,\n",
      "          -18.3159],\n",
      "         [-18.7620, -18.6450, -18.7883, -18.8117,  -0.8162,  -1.3775,  -2.6864,\n",
      "           -3.4523,  -1.1282,  -0.2184,  -0.3621,   0.9205,  -2.9299,  -3.5531,\n",
      "           -1.3846,   0.6252,   7.9505,  -0.6458,  -5.6197,  -1.6017,  -0.1716,\n",
      "            0.1989,  -5.5975,  -4.3110,  -8.2363, -26.9518, -24.3963, -21.0155,\n",
      "          -18.3532, -18.5675, -18.5852, -18.7949, -18.7387, -18.4638, -18.5972,\n",
      "          -18.6543, -18.7179, -18.5404, -18.6242, -18.6578, -18.5608, -18.4569,\n",
      "          -18.6508, -18.6003, -18.6535, -18.7908, -18.6988, -18.5729, -18.7188,\n",
      "          -18.7206, -18.7868, -18.7211, -18.6735, -18.7122, -18.7668, -18.8629,\n",
      "          -18.6954, -18.7511, -18.4883, -18.6831, -18.6387, -18.7376, -18.7119,\n",
      "          -18.8450]]], device='cuda:0'), structure_logits=tensor([[[24.0083, 23.3656, 27.2712,  ..., 23.0110, 20.4230, 25.4696],\n",
      "         [25.5255, 25.4624, 25.2477,  ..., 25.2815, 25.0529, 25.0369],\n",
      "         [29.1018, 25.6902, 23.0858,  ..., 26.1237, 26.3236, 24.6597],\n",
      "         [28.9306, 26.6349, 26.4169,  ..., 26.4133, 26.8507, 27.7038]]],\n",
      "       device='cuda:0'), secondary_structure_logits=tensor([[[-14.6039, -14.5778, -14.5726,  -3.0977,  -0.8239,  -5.4925,  -1.9526,\n",
      "            2.2546,  -3.8221,   0.3640,   1.3179],\n",
      "         [-15.2675, -15.2096, -15.2480,  -4.1084,  -0.1440,  -5.4120,  -2.7797,\n",
      "            3.5394,  -4.2331,  -0.7220,   1.2852],\n",
      "         [-14.5730, -14.5764, -14.5682,  -3.1754,  -0.7207,  -4.3030,  -2.4185,\n",
      "            3.0533,  -2.6936,  -1.6474,   0.4577],\n",
      "         [-15.5222, -15.4978, -15.4951,  -3.0732,  -0.0855,  -5.1873,  -1.9709,\n",
      "            3.1002,  -4.4646,  -1.3744,   0.1255]]], device='cuda:0'), sasa_logits=tensor([[[-11.6781, -11.5056, -11.5838,  -2.2567,  -1.5531,  -0.6364,  -0.4990,\n",
      "           -0.3424,   0.0440,   0.1206,  -0.9770,  -2.0901,  -2.8259,  -3.6447,\n",
      "           -4.9203,  -5.2884,  -6.0924,  -6.9175,  -8.1886],\n",
      "         [-19.9654, -19.8646, -20.0075,  -2.4089,  -3.4763,  -3.8271,  -4.3866,\n",
      "           -4.8071,  -4.9061,  -4.8744,  -4.8731,  -4.5728,  -4.4213,  -4.1352,\n",
      "           -4.1746,  -4.5904,  -4.9029,  -5.3221,  -6.5248],\n",
      "         [-19.0611, -19.0004, -18.9960,  -3.3238,  -3.6014,  -3.6583,  -4.1127,\n",
      "           -4.3571,  -4.2036,  -3.5715,  -3.1611,  -3.1801,  -3.7063,  -4.2918,\n",
      "           -5.2585,  -5.5561,  -5.5688,  -5.9042,  -6.6974],\n",
      "         [-20.7915, -20.6934, -20.7451,  -5.5260,  -5.6989,  -5.4070,  -5.4569,\n",
      "           -5.5403,  -5.3554,  -4.8557,  -4.2235,  -3.4528,  -3.0104,  -2.5078,\n",
      "           -2.5678,  -2.9167,  -3.4477,  -4.0596,  -5.2845]]], device='cuda:0'), function_logits=tensor([[[[-6.2599, -6.3590, -5.9904,  ..., 40.7326, 40.4593, 41.9860],\n",
      "          [-5.9203, -6.1103, -6.1258,  ..., 41.2638, 41.7063, 40.8945],\n",
      "          [-6.8201, -6.6600, -6.6366,  ..., 42.8080, 40.3407, 35.2904],\n",
      "          ...,\n",
      "          [-6.8138, -6.5468, -6.9576,  ..., 39.7397, 39.5143, 36.9787],\n",
      "          [-6.3742, -6.2851, -6.2829,  ..., 39.4606, 40.2904, 38.6962],\n",
      "          [-6.5269, -6.3560, -6.4416,  ..., 39.9169, 39.5316, 39.8553]],\n",
      "\n",
      "         [[-5.8118, -5.8592, -5.5351,  ..., 38.9725, 39.5466, 39.9652],\n",
      "          [-5.4143, -5.6414, -5.6963,  ..., 39.8350, 39.7378, 39.8861],\n",
      "          [-6.2433, -6.1357, -6.0436,  ..., 42.0070, 39.1906, 34.2803],\n",
      "          ...,\n",
      "          [-6.3494, -6.0524, -6.4879,  ..., 37.6772, 37.4591, 36.4228],\n",
      "          [-5.9014, -5.7205, -5.7600,  ..., 37.9126, 38.5018, 37.2063],\n",
      "          [-5.8626, -5.6816, -5.8424,  ..., 39.0519, 38.2458, 38.4069]],\n",
      "\n",
      "         [[-5.3022, -5.4242, -5.0118,  ..., 37.9033, 38.6869, 39.8670],\n",
      "          [-4.9217, -5.1872, -5.1694,  ..., 39.3765, 39.4511, 39.5412],\n",
      "          [-5.7311, -5.5602, -5.4643,  ..., 41.1519, 37.5849, 34.7452],\n",
      "          ...,\n",
      "          [-5.8296, -5.5473, -5.9589,  ..., 36.1851, 37.7714, 35.3125],\n",
      "          [-5.4924, -5.3003, -5.2929,  ..., 36.7786, 37.1191, 35.9732],\n",
      "          [-5.3749, -5.1550, -5.2929,  ..., 38.0694, 37.9238, 38.2512]],\n",
      "\n",
      "         [[-7.0649, -7.1126, -6.7480,  ..., 30.8430, 31.6933, 31.6022],\n",
      "          [-6.6959, -6.9285, -6.9691,  ..., 31.8883, 32.7424, 31.8498],\n",
      "          [-7.6650, -7.4773, -7.4600,  ..., 33.8123, 30.4055, 24.8689],\n",
      "          ...,\n",
      "          [-7.6976, -7.4967, -7.9081,  ..., 31.0331, 29.3269, 28.5460],\n",
      "          [-7.2621, -7.1683, -7.1235,  ..., 29.3272, 29.9833, 28.5153],\n",
      "          [-7.3620, -7.1113, -7.3166,  ..., 30.4390, 30.4385, 30.0731]]]],\n",
      "       device='cuda:0'), residue_logits=tensor([[[ -0.3568,   0.2694,  -0.3122,  ..., -16.4309, -15.8039, -14.4512],\n",
      "         [ -0.2068,   0.1074,  -0.1303,  ..., -15.4325, -14.8886, -14.8799],\n",
      "         [ -0.5198,   0.1270,  -0.2663,  ..., -15.5042, -14.7191, -14.0274],\n",
      "         [ -0.3389,   0.1020,  -0.0665,  ..., -15.2908, -14.9418, -12.7222]]],\n",
      "       device='cuda:0'), embeddings=tensor([[[ 131.4066,  -41.1126,  229.0576,  ...,   12.3590,   44.5290,\n",
      "            -9.5239],\n",
      "         [ 147.6538,  -39.6118,   64.1233,  ...,  184.4493,   15.9873,\n",
      "            21.7239],\n",
      "         [ 119.2787,  -25.8110,  199.2384,  ...,  160.7945,   69.0046,\n",
      "            98.2880],\n",
      "         [ 152.9911, -128.5386,   73.6433,  ...,   50.0046,  -95.7689,\n",
      "           131.8537]]], device='cuda:0'))\n"
     ]
    }
   ],
   "source": [
    "# Dummy data\n",
    "sequences = [\"GLVM\", \"MGLV\"]\n",
    "\n",
    "# Initialize the model with the ESM3InferenceClient instance\n",
    "model_instance = ESM3ForTokenClassification(model=model, num_labels=1)\n",
    "\n",
    "# Forward pass with dummy data\n",
    "tokens = torch.tensor(model_instance.tokenizer.encode((\"\", \"GLVM\"))).unsqueeze(0).to(model_instance.device)\n",
    "average_plddt = torch.tensor([70.0], dtype=next(model_instance.esm3.parameters()).dtype).to(model_instance.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model_instance.esm3(sequence_tokens=tokens, average_plddt=average_plddt)\n",
    "    print(\"Type of outputs:\", type(outputs))\n",
    "    print(\"Attributes of outputs:\", dir(outputs))\n",
    "    if isinstance(outputs, dict):\n",
    "        print(\"Keys in ESMOutput:\", outputs.keys())\n",
    "    elif isinstance(outputs, tuple):\n",
    "        print(f\"Number of elements in ESMOutput tuple: {len(outputs)}\")\n",
    "    print(\"Contents of outputs:\", outputs)\n",
    "    if hasattr(outputs, 'last_hidden_state'):\n",
    "        print(\"Last Hidden State:\", outputs.last_hidden_state)\n",
    "    if hasattr(outputs, 'hidden_states'):\n",
    "        print(\"Hidden States:\", outputs.hidden_states)\n",
    "    if hasattr(outputs, 'logits'):\n",
    "        print(\"Logits:\", outputs.logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "74c97779-a6f6-4499-8d47-8ed7a669e17f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'EsmSequenceTokenizer' object has no attribute 'batch_encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Instantiate and train the model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m MaskedRegressTrainer(\n\u001b[0;32m----> 3\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel_init_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m,   \u001b[38;5;66;03m# Use the ESM3-based model for token classification\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m      5\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_ds,\n\u001b[1;32m      6\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtest_ds,\n\u001b[1;32m      7\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mcollator_fn,\n\u001b[1;32m      8\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics_regr\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m     12\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m, in \u001b[0;36mmodel_init_1\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_init_1\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mESM3ForTokenClassification\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 16\u001b[0m, in \u001b[0;36mESM3ForTokenClassification.__init__\u001b[0;34m(self, model, num_labels)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Tokenize a dummy input to get the embedding dimension\u001b[39;00m\n\u001b[1;32m     15\u001b[0m dummy_batch \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGLVM\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m---> 16\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode\u001b[49m(dummy_batch)  \u001b[38;5;66;03m# Adjust based on actual API\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Inspect what `batch_encode` returns\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncoded tokens (dummy): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EsmSequenceTokenizer' object has no attribute 'batch_encode'"
     ]
    }
   ],
   "source": [
    "# Instantiate and train the model\n",
    "trainer = MaskedRegressTrainer(\n",
    "    model=model_init_1(),   # Use the ESM3-based model for token classification\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    data_collator=collator_fn,\n",
    "    compute_metrics=compute_metrics_regr\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned weights\n",
    "fine_tuned_model_path = os.path.join(efs_model_path, \"fine_tuned_sema_3_ESM3.pth\")\n",
    "torch.save(trainer.model.state_dict(), fine_tuned_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6887fef-7c0b-4353-9ed1-c40d8fa60fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
